# ğŸŒ¿ Plant Care Chatbot with Amazon Bedrock

## ğŸ” Overview
A natural language chatbot that provides accurate and personalized plant care advice by retrieving relevant information from a custom-built knowledge base. This project leverages **Amazon Bedrock** Agents with **retrieval-augmented generation (RAG)** to enable seamless Q&A over user-curated content.

---

## ğŸ” Problem

Most plant care advice online is **fragmented**, **overly generalized**, or **buried in forums and blog posts**. You often have to sift through dozens of pages to answer simple but specific questions like:

- How much light does a pothos actually need?
- Why are my monstera leaves curling?
- What soil mix should I use for succulents?

Search results may contradict each other, lack scientific grounding, or assume conditions that donâ€™t apply to your home. The result is confusion, trial-and-error, and unhealthy plants.

---

## ğŸ’¡ Solution

I built a **Retrieval-Augmented Generation (RAG) chatbot** using **Amazon Bedrock** that gives accurate, consistent answers to plant care questions â€” sourced from structured, vetted horticultural documents.

Instead of scraping the internet, this chatbot retrieves relevant content from a **predefined plant care knowledge base** and uses a foundation model to generate clear, helpful responses.

---

## ğŸ§  How It Works

<img width="822" height="427" alt="Screenshot 2025-07-27 at 4 33 33â€¯PM" src="https://github.com/user-attachments/assets/2a82a728-983a-416f-bf2b-b1d1153fa5ee" />

- ğŸ“„ **Amazon S3** holds structured plant care docs (PDFs, guides, etc.)
- ğŸ§  **Titan Text Embeddings v2** converts documents into semantic vectors
- ğŸ” **Amazon OpenSearch Serverless** stores and retrieves embeddings
- ğŸ¤– **Amazon Bedrock Agent** (LLaMA 3.3 70B) answers questions using retrieved content
This RAG architecture ensures that every answer is grounded in reliable source material â€” no hallucination, no guesswork.


---
## ğŸ’¬ Example Interactions

<img width="303" height="497" alt="Screenshot 2025-07-27 at 3 55 18â€¯PM" src="https://github.com/user-attachments/assets/82a252b9-b30d-444f-a7d7-e1891c292936" />

In this example, the chatbot answers â€œWhat causes yellowing leaves in pothos?â€ with a clear, in-depth explanation thatâ€™s actually useful. Instead of just listing possible causes, it explains the most common onesâ€”like overwatering, root rot, and nutrient deficienciesâ€”and even tells you how to check for root rot by looking at the condition of the roots. Unlike a regular Google search, which would return a list of blogs, forums, and websites you have to sift through yourself, this chatbot delivers a direct, trustworthy response in one place. It also cites the exact sources it used, so you know where the information came from and can get further details if needed.

---


## ğŸ§° Technologies Used

| Component             | Service / Model                     |
|----------------------|--------------------------------------|
| Foundation Model     | Meta LLaMA 3.3 70B (Bedrock)         |
| Embedding Model      | Titan Text Embeddings v2             |
| Vector Store         | Amazon OpenSearch Serverless         |
| File Storage         | Amazon S3                            |
| Orchestration Layer  | Amazon Bedrock Agent + Knowledge Base|



---

## ğŸŒ± Why This Matters

By combining **semantic search** with **natural language generation**, this chatbot makes plant care knowledge accessible, specific, and trustworthy â€” all without requiring users to know the right keywords or source.

Itâ€™s a practical example of how RAG can be used to deliver **niche, reliable AI answers** in a noisy online world.

---

### ğŸ“ What I Learned

Through this project, I gained hands-on experience with building a Retrieval-Augmented Generation (RAG) chatbot using Amazon Bedrock and related AWS services. I learned how to:

- Set up a **Knowledge Base in Amazon Bedrock**, connecting it to documents stored in Amazon S3
- Use **Titan Text Embeddings v2** to convert text into semantic vector representations
- Configure and deploy **Amazon OpenSearch Serverless** as a vector store for fast, meaningful retrieval
- Choose and apply a suitable **foundation model** (LLaMA 3.3 70B Instruct) for generating responses
- Understand the difference between **on-demand vs. provisioned inference**, and how to troubleshoot model invocation issues
- Enable **retrieval-augmented generation** to ground responses in specific, trusted documents and reduce hallucination
- Improve response quality through **prompt engineering** and plan for **fine-tuning** to better handle edge cases

Overall, this project gave me a practical understanding of how to integrate multiple AWS services to build a scalable, domain-specific chatbot.

### ğŸ”„ Next Steps

To improve the chatbotâ€™s performance even further, the next phase will focus on:
- **Prompt engineering** to guide tone, completeness, and contextual accuracy.
- **Fine-tuning** on more targeted plant care data to handle edge cases like pest issues or seasonal needs.
- Exploring **multi-turn conversation handling** so the chatbot can follow up or clarify when users ask complex or vague questions.



## ğŸ“ Related Links

- ğŸ” [What is Retrieval-Augmented Generation (RAG)?](https://www.pinecone.io/learn/retrieval-augmented-generation/)
- ğŸ¤– [Amazon Bedrock Overview](https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html)
- ğŸ§  [Metaâ€™s LLaMA Models](https://ai.meta.com/llama/)
- ğŸ“Š [Titan Text Embeddings v2 (AWS)](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html#foundation-models-titan)
- ğŸ” [Amazon OpenSearch Serverless](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless.html)
